{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloning via Backup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis.gis import GIS, Item\n",
    "import keyring\n",
    "import pathlib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_itemId = '4e373608ba444a639bfaa0c893d3d99d'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a profile to store credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GIS(username='username', password='password',\n",
    "                     profile='example_profile')\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to our Organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = GIS(profile=\"dyaw_geosaurus\")\n",
    "target = GIS(profile=\"dyaw_Arch\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_item = Item(source, source_itemId)\n",
    "\n",
    "source_item"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = source_item.download(r\"C:\\temp\\clone_via_backup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Thumbnail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thumbnail = source_item.download_thumbnail(r\"C:\\temp\\clone_via_backup\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = source_item.download_metadata(r\"C:\\temp\\clone_via_backup\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering + Downloading Item Properties"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the properties we want to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_PROPERTIES_PORTAL_ITEM_ADD = [\n",
    "        \"type\",\n",
    "        \"dataUrl\",\n",
    "        \"filename\",\n",
    "        \"typeKeywords\",\n",
    "        \"description\",\n",
    "        \"title\",\n",
    "        \"text\",\n",
    "        \"tags\",\n",
    "        \"snippet\",\n",
    "        \"extent\",\n",
    "        \"spatialReference\",\n",
    "        \"accessInformation\",\n",
    "        \"licenseInfo\",\n",
    "        \"culture\",\n",
    "        \"commentsEnabled\",\n",
    "        \"culture\",\n",
    "        \"overwrite\",\n",
    "        \"url\",\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grabbing the properties we want to keep\n",
    "Using sets to compare our item's vars to VALID_PROPERTIES_PORTAL_ITEM_ADD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_properties = {\n",
    "        k: vars(source_item)[k]\n",
    "        for k in set(VALID_PROPERTIES_PORTAL_ITEM_ADD) &\n",
    "        set(vars(source_item).keys())\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's save that to JSON for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_properties_path = pathlib.PurePath(r\"C:\\temp\\clone_via_backup\", \"add_properties.json\")\n",
    "\n",
    "with open(add_properties_path,\"w\") as outfile:\n",
    "    json.dump(add_properties, outfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the Item"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing the properties and data we saved earlier as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(add_properties_path, \"r\") as json_data:\n",
    "    add_properties = json.load(json_data)\n",
    "    \n",
    "with open(data, \"r\") as json_data:\n",
    "    new_data = json.load(json_data)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = target.content.add(\n",
    "    item_properties=add_properties,\n",
    "    thumbnail = thumbnail,\n",
    "    metadata = metadata,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.update(\n",
    "    data = new_data,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's throw this into a function\n",
    "To make it easier for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_PROPERTIES_PORTAL_ITEM_ADD = [\n",
    "        \"type\",\n",
    "        \"dataUrl\",\n",
    "        \"filename\",\n",
    "        \"typeKeywords\",\n",
    "        \"description\",\n",
    "        \"title\",\n",
    "        \"text\",\n",
    "        \"tags\",\n",
    "        \"snippet\",\n",
    "        \"extent\",\n",
    "        \"spatialReference\",\n",
    "        \"accessInformation\",\n",
    "        \"licenseInfo\",\n",
    "        \"culture\",\n",
    "        \"commentsEnabled\",\n",
    "        \"culture\",\n",
    "        \"overwrite\",\n",
    "        \"url\",\n",
    "    ]\n",
    "\n",
    "def migrate_via_backup(itemId: str, source: GIS, target: GIS):\n",
    "    \n",
    "    export_dir = pathlib.PurePath(r\"C:\\temp\\clone_via_backup\", itemId)\n",
    "    \n",
    "    # Part 1: Download the artifacts from the source\n",
    "    \n",
    "    # Grab the item from the source\n",
    "    source_item = Item(source, itemId)\n",
    "    \n",
    "    # Download the item from the source\n",
    "    data = source_item.download(export_dir)\n",
    "    \n",
    "    # Download the thumbnail from the source\n",
    "    thumbnail = source_item.download_thumbnail(export_dir)\n",
    "    \n",
    "    # Download the metadata from the source\n",
    "    metadata = source_item.download_metadata(export_dir)\n",
    "    \n",
    "    # Grab the properties of the item from the source\n",
    "    add_properties = {\n",
    "        k: vars(source_item)[k]\n",
    "        for k in set(VALID_PROPERTIES_PORTAL_ITEM_ADD) &\n",
    "        set(vars(source_item).keys())\n",
    "    }\n",
    "    \n",
    "    # Save the properties to a file\n",
    "    add_properties_path = pathlib.PurePath(export_dir, \"add_properties.json\")\n",
    "\n",
    "    with open(add_properties_path,\"w\") as outfile:\n",
    "        json.dump(add_properties, outfile)\n",
    "    \n",
    "    # Part 2: Add the item to the target\n",
    "        \n",
    "    # Add the item to the target\n",
    "    with open(add_properties_path, \"r\") as json_data:\n",
    "        add_properties = json.load(json_data)\n",
    "    \n",
    "    with open(data, \"r\") as json_data:\n",
    "        new_data = json.load(json_data)    \n",
    "    \n",
    "    # Create the item on the target\n",
    "    result = target.content.add(\n",
    "        item_properties=add_properties,\n",
    "        thumbnail = thumbnail,\n",
    "        metadata = metadata,\n",
    "        #owner = source_item.owner,\n",
    "        #folder = source_item.ownerFolder\n",
    "    )\n",
    "    \n",
    "    # Add the data to the item on the target\n",
    "    result.update(\n",
    "        data = new_data,\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look the result\n",
    "### Something is missing here... where's the dependencies? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try that again\n",
    "This time, we'll find and take care of the dependencies first."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the dependencies from the item data\n",
    "\n",
    "The below function will handle finding strings, using regex, recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "\n",
    "def find_regex(i: dict or list or str, regex: str, res=[]):\n",
    "    \"\"\"\n",
    "    Takes a dict with nested lists and dicts,\n",
    "    and searches all dicts for a key of the field\n",
    "    provided.\n",
    "    \"\"\"\n",
    "    if isinstance(i, dict):\n",
    "        for v in i.values():\n",
    "            find_regex(v, regex, res)\n",
    "    elif isinstance(i, list):\n",
    "        for v in i:\n",
    "            find_regex(v, regex, res)\n",
    "    elif isinstance(i, str):\n",
    "        matches = re.findall(regex, i, re.MULTILINE)\n",
    "        if matches:\n",
    "            res.append(matches)\n",
    "    # Flattening list of lists\n",
    "    results = list(itertools.chain(*res))\n",
    "    # Removing duplicates\n",
    "    results = list(OrderedDict.fromkeys(results))\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here's a regex string to find GUIDs and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_guid = r\"[0-9a-f]{8}[0-9a-f]{4}[1-5][0-9a-f]{3}[89ab][0-9a-f]{3}[0-9a-f]{12}\"\n",
    "\n",
    "regex_url = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we have what we need to find all itemIds in the JSON\n",
    "Remember, itemIds are GUIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemIds = find_regex(new_data, regex_guid, [])\n",
    "itemIds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### but wait... that itemId does not exist in our org!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for itemId in itemIds:\n",
    "    try: \n",
    "        target.content.get(itemId)\n",
    "        print(f'{itemId} found in target')\n",
    "    except:\n",
    "        print(f'{itemId} not found in target')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And what about the dependency's dependencies? And their dependencies?  \n",
    "\n",
    "### Let's make a function to find dependencies, **recursively** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dependencies(itemId: str, source_portal: GIS, dependencies_from_source=[]):\n",
    "    \n",
    "    item = source.content.get(itemId)\n",
    "    item_data = item.get_data()\n",
    "    \n",
    "    itemIds = find_regex(item_data, regex_guid, [])\n",
    "    \n",
    "    for itemId in itemIds if len(itemIds) > 0 else []:\n",
    "        try: \n",
    "            source_portal.content.get(itemId)\n",
    "            dependencies_from_source.append(itemId)\n",
    "            find_dependencies(itemId, source, dependencies_from_source)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return dependencies_from_source"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But we don't need to migrate Esri-owned items!\n",
    "This won't work, and we don't need to do it. For content owned by Esri built-in users, itemIds are the same across organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESRI_BUILTIN_USERS = [\n",
    "    \"esri\",\n",
    "    \"esri_apps\",\n",
    "    \"esri_ar\",\n",
    "    \"esri_boundaries\",\n",
    "    \"esri_bs\",\n",
    "    \"esri_ca\",\n",
    "    \"esri_cs\",\n",
    "    \"esri_da\",\n",
    "    \"esri_de\",\n",
    "    \"esri_demographics\",\n",
    "    \"esri_el\",\n",
    "    \"esri_en\",\n",
    "    \"esri_es\",\n",
    "    \"esri_et\",\n",
    "    \"esri_fi\",\n",
    "    \"esri_fr\",\n",
    "    \"esri_he\",\n",
    "    \"esri_hi\",\n",
    "    \"esri_hk\",\n",
    "    \"esri_hr\",\n",
    "    \"esri_hu\",\n",
    "    \"esri_id\",\n",
    "    \"esri_ind\",\n",
    "    \"esri_it\",\n",
    "    \"esri_ja\",\n",
    "    \"esri_ko\",\n",
    "    \"esri_livingatlas\",\n",
    "    \"esri_lt\",\n",
    "    \"esri_lv\",\n",
    "    \"esri_nav\",\n",
    "    \"esri_nav\",\n",
    "    \"esri_nb\",\n",
    "    \"esri_nl\",\n",
    "    \"esri_pl\",\n",
    "    \"esri_po\",\n",
    "    \"esri_pt\",\n",
    "    \"esri_ro\",\n",
    "    \"esri_ru\",\n",
    "    \"esri_sl\",\n",
    "    \"esri_sr\",\n",
    "    \"esri_sv\",\n",
    "    \"esri_th\",\n",
    "    \"esri_tr\",\n",
    "    \"esri_tw\",\n",
    "    \"esri_vi\",\n",
    "    \"esri_zh\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we'll pull this all together and try again"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we'll find everything we need to clone\n",
    "These, along with the source item, will make a list of everything we want to migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_item = Item(source, source_itemId)\n",
    "\n",
    "all_items_to_migrate = find_dependencies(itemId = source_itemId, source_portal = source) + [source_itemId]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we'll migrate all of this\n",
    "\n",
    "We'll throw the old and new itemIds into a dictionary we'll use in the next step\n",
    "\n",
    "#### One thing...\n",
    "\n",
    "We're going to use content.clone_items to clone the hosted feature services. Backing up these can be done, but is a bit more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_dict = {}\n",
    "\n",
    "for itemId in all_items_to_migrate:\n",
    "    # Grab the item object\n",
    "    item = source.content.get(itemId)\n",
    "    \n",
    "    if item.owner in ESRI_BUILTIN_USERS:\n",
    "        continue\n",
    "    \n",
    "    print(f\"Migrating {itemId} {item.type}\")\n",
    "    if item.type != 'Feature Service':\n",
    "        result = migrate_via_backup(itemId, source, target)\n",
    "    else:\n",
    "        result = target.content.clone_items(\n",
    "            items = [item],\n",
    "            search_existing_items = False\n",
    "            )[0]\n",
    "    \n",
    "    replacement_dict[itemId] = result.id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, we need to update JSON in the target\n",
    "To have correct itemIds for migrated dependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But first, some more helper code\n",
    "This code will traverse the item's JSON and will use the itemId replacement dictionary to find and replace itemIds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_deep(data, a, b):\n",
    "    \"\"\"Finds all instances of a string in a nested data structure and replaces them with b\n",
    "    Args:\n",
    "        data (dict, list, or string): Object with text to be replaced\n",
    "        a (str): Text to find\n",
    "        b (any): Text to replace\n",
    "\n",
    "    Returns:\n",
    "        dict, lis, or string: Data with text replaced\n",
    "    \"\"\"\n",
    "    if isinstance(data, str):\n",
    "        return data.replace(a, b)\n",
    "    elif isinstance(data, dict):\n",
    "        return {k: replace_deep(v, a, b) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [replace_deep(v, a, b) for v in data]\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, replace old itemIds with new itemIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in replacement_dict.items():\n",
    "    target_item = target.content.get(v)\n",
    "    print(target_item.homepage)\n",
    "    target_item_data = target_item.get_data()\n",
    "    for k, v in replacement_dict.items():\n",
    "        target_item_data = replace_deep(target_item_data, k, v)\n",
    "    target_item.update(data = target_item_data)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we can admire our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = replacement_dict[source_itemId]\n",
    "target.content.get(final_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c7e8831da8dac8d008a073f73e017a2094033623d89649309389285d33ff6f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
